{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Analytics - AWS 8/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Exercicio Intro\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 21:41:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|  Frances Bennet|\n",
      "|   Jamie Russell|\n",
      "|  Edward Kistler|\n",
      "|   Sheila Maurer|\n",
      "|Donald Golightly|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes = spark.read.csv('nomes_aleatorios.txt')\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            Nomes|\n",
      "+-----------------+\n",
      "|   Frances Bennet|\n",
      "|    Jamie Russell|\n",
      "|   Edward Kistler|\n",
      "|    Sheila Maurer|\n",
      "| Donald Golightly|\n",
      "|       David Gray|\n",
      "|      Joy Bennett|\n",
      "|      Paul Kriese|\n",
      "|Berniece Ornellas|\n",
      "|    Brian Farrell|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\",\"Nomes\")\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nomes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, rand, lit\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\"Escolaridade\", \\\n",
    "    when(rand() < 0.22, lit(\"Fundamental\")) \\\n",
    "    .when(rand() < 0.44, lit(\"Médio\")) \\\n",
    "    .otherwise(lit(\"Superior\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|            Nomes|Escolaridade|\n",
      "+-----------------+------------+\n",
      "|   Frances Bennet|       Médio|\n",
      "|    Jamie Russell|       Médio|\n",
      "|   Edward Kistler|       Médio|\n",
      "|    Sheila Maurer|       Médio|\n",
      "| Donald Golightly|    Superior|\n",
      "|       David Gray|    Superior|\n",
      "|      Joy Bennett|    Superior|\n",
      "|      Paul Kriese|       Médio|\n",
      "|Berniece Ornellas| Fundamental|\n",
      "|    Brian Farrell|    Superior|\n",
      "|   Kara Mcelwaine|       Médio|\n",
      "|    Tracy Herring|       Médio|\n",
      "|  Howard Lazarine|    Superior|\n",
      "|     Leroy Strahl|    Superior|\n",
      "|     Ernest Hulet|    Superior|\n",
      "|     David Medina|       Médio|\n",
      "|   Lorenzo Woodis|       Médio|\n",
      "|      Page Marthe|       Médio|\n",
      "|   Herbert Morris| Fundamental|\n",
      "|      Albert Leef|       Médio|\n",
      "+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nomes: string (nullable = true)\n",
      " |-- Escolaridade: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nomes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+\n",
      "|            Nomes|Escolaridade|     Pais|\n",
      "+-----------------+------------+---------+\n",
      "|   Frances Bennet|       Médio|  Bolívia|\n",
      "|    Jamie Russell|       Médio| Suriname|\n",
      "|   Edward Kistler|       Médio|Venezuela|\n",
      "|    Sheila Maurer|       Médio|  Equador|\n",
      "| Donald Golightly|    Superior|Argentina|\n",
      "|       David Gray|    Superior|Argentina|\n",
      "|      Joy Bennett|    Superior|   Brasil|\n",
      "|      Paul Kriese|       Médio|Argentina|\n",
      "|Berniece Ornellas| Fundamental| Suriname|\n",
      "|    Brian Farrell|    Superior|Argentina|\n",
      "|   Kara Mcelwaine|       Médio|Argentina|\n",
      "|    Tracy Herring|       Médio| Suriname|\n",
      "|  Howard Lazarine|    Superior|  Bolívia|\n",
      "|     Leroy Strahl|    Superior|  Bolívia|\n",
      "|     Ernest Hulet|    Superior|Argentina|\n",
      "|     David Medina|       Médio|Argentina|\n",
      "|   Lorenzo Woodis|       Médio| Suriname|\n",
      "|      Page Marthe|       Médio|Argentina|\n",
      "|   Herbert Morris| Fundamental| Suriname|\n",
      "|      Albert Leef|       Médio|  Uruguai|\n",
      "+-----------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "paises_amesul = [\"Argentina\", \"Bolívia\", \"Brasil\", \"Chile\", \"Colômbia\", \"Equador\",\n",
    "                      \"Guiana\", \"Paraguai\", \"Peru\", \"Suriname\", \"Uruguai\", \"Venezuela\"]\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\"Pais\",\n",
    "    when((rand() * 12).cast(\"int\") == 11, lit(paises_amesul[11])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 10, lit(paises_amesul[10])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 9, lit(paises_amesul[9])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 8, lit(paises_amesul[8])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 7, lit(paises_amesul[7])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 6, lit(paises_amesul[6])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 5, lit(paises_amesul[5])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 4, lit(paises_amesul[4])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 3, lit(paises_amesul[3])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 2, lit(paises_amesul[2])) \\\n",
    "    .when((rand() * 12).cast(\"int\") == 1, lit(paises_amesul[1])) \\\n",
    "    .otherwise(lit(paises_amesul[0]))\n",
    ")\n",
    "\n",
    "df_nomes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. \n",
    "\n",
    "\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+---------+-------------+\n",
      "|            Nomes|Escolaridade|     Pais|AnoNascimento|\n",
      "+-----------------+------------+---------+-------------+\n",
      "|   Frances Bennet|       Médio|  Bolívia|         1976|\n",
      "|    Jamie Russell|       Médio| Suriname|         1999|\n",
      "|   Edward Kistler|       Médio|Venezuela|         1988|\n",
      "|    Sheila Maurer|       Médio|  Equador|         1967|\n",
      "| Donald Golightly|    Superior|Argentina|         1992|\n",
      "|       David Gray|    Superior|Argentina|         1996|\n",
      "|      Joy Bennett|    Superior|   Brasil|         1998|\n",
      "|      Paul Kriese|       Médio|Argentina|         2008|\n",
      "|Berniece Ornellas| Fundamental| Suriname|         1987|\n",
      "|    Brian Farrell|    Superior|Argentina|         2006|\n",
      "|   Kara Mcelwaine|       Médio|Argentina|         1984|\n",
      "|    Tracy Herring|       Médio| Suriname|         1993|\n",
      "|  Howard Lazarine|    Superior|  Bolívia|         1997|\n",
      "|     Leroy Strahl|    Superior|  Bolívia|         1980|\n",
      "|     Ernest Hulet|    Superior|Argentina|         1974|\n",
      "|     David Medina|       Médio|Argentina|         1996|\n",
      "|   Lorenzo Woodis|       Médio| Suriname|         1961|\n",
      "|      Page Marthe|       Médio|Argentina|         2006|\n",
      "|   Herbert Morris| Fundamental| Suriname|         1963|\n",
      "|      Albert Leef|       Médio|  Uruguai|         1952|\n",
      "+-----------------+------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, lit\n",
    "\n",
    "# valores aleatórios entre 1945 e 2010\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", \n",
    "                              (lit(1945) + (rand() * (2010 - 1945))).cast(\"int\"))\n",
    "\n",
    "df_nomes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|           Nomes|AnoNascimento|\n",
      "+----------------+-------------+\n",
      "|     Paul Kriese|         2008|\n",
      "|   Brian Farrell|         2006|\n",
      "|     Page Marthe|         2006|\n",
      "|Wallace Mitchell|         2006|\n",
      "|        Mary Lee|         2005|\n",
      "|    Roxie Bernal|         2007|\n",
      "|  Wilfredo Grant|         2009|\n",
      "|     Jessie Jean|         2008|\n",
      "|      Ned Tester|         2004|\n",
      "|   Lynne Dustman|         2009|\n",
      "+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleção de pessoas que nasceram neste século\n",
    "df_select = df_nomes.select(\"Nomes\", \"AnoNascimento\").where(df_nomes.AnoNascimento >= 2000)\n",
    "\n",
    "# Imprimir nomes e ano de nascimento\n",
    "df_select.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "df_nomes.createOrReplaceTempView (\"pessoas\")\n",
    "\n",
    "spark.sql(\"select * from pessoas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|           Nomes|AnoNascimento|\n",
      "+----------------+-------------+\n",
      "|     Paul Kriese|         2008|\n",
      "|   Brian Farrell|         2006|\n",
      "|     Page Marthe|         2006|\n",
      "|Wallace Mitchell|         2006|\n",
      "|        Mary Lee|         2005|\n",
      "|    Roxie Bernal|         2007|\n",
      "|  Wilfredo Grant|         2009|\n",
      "|     Jessie Jean|         2008|\n",
      "|      Ned Tester|         2004|\n",
      "|   Lynne Dustman|         2009|\n",
      "+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrar o dataframe como uma view temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Selecionar as pessoas que nasceram neste século\n",
    "query_birth = spark.sql(\"\"\"\n",
    "    select Nomes, AnoNascimento\n",
    "    from pessoas\n",
    "    where AnoNascimento >= 2000\n",
    "    limit 10\n",
    "\"\"\")\n",
    "\n",
    "# Imprime o resultado\n",
    "query_birth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o método select do Dataframe df_nomes, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de pessoas da geração Millennials: 2311022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Filtrar apenas os nascidos entre 1980 e 1994\n",
    "df_millennials = df_nomes.select(\"*\").where((df_nomes.AnoNascimento >= 1980) & (df_nomes.AnoNascimento <= 1994))\n",
    "\n",
    "# Contagem do número de pessoas da geração Millennials\n",
    "gen_millennials = df_millennials.select(count(\"*\")).collect()[0][0]\n",
    "\n",
    "# Exibir a contagem de pessoas da gen millenials\n",
    "print(\"Contagem de pessoas da geração Millennials:\", gen_millennials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita o processo da Pergunta 8 utilizando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pessoas da geração Millennials: 2311022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# criação de uma view temporária\n",
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "# Contar o numero de pessoas da geração millenials\n",
    "query_millenials = \"\"\"\n",
    "    select count(*) as CountMille\n",
    "    from pessoas\n",
    "    where AnoNascimento between 1980 and 1994\n",
    "\"\"\"\n",
    "\n",
    "# Executa a consulta SQL\n",
    "resultado = spark.sql(query_millenials)\n",
    "\n",
    "# Extrai o resultado\n",
    "total = resultado.collect()[0][\"CountMille\"]\n",
    "\n",
    "print(\"Pessoas da geração Millennials:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex. 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "\n",
    "- Baby Boomers – nascidos entre 1944 e 1964;\n",
    "\n",
    "- Geração X – nascidos entre 1965 e 1979;\n",
    "\n",
    "- Millennials (Geração Y) – nascidos entre 1980 e 1994;\n",
    "\n",
    "- Geração Z – nascidos entre 1995 e 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+\n",
      "|     pais|     Geracao|Quantidade|\n",
      "+---------+------------+----------+\n",
      "|Argentina|Baby Boomers|   1183221|\n",
      "|Argentina|   Geração X|    885391|\n",
      "|Argentina|   Geração Y|    887779|\n",
      "|Argentina|   Geração Z|    887036|\n",
      "|  Bolívia|Baby Boomers|    107314|\n",
      "|  Bolívia|   Geração X|     80408|\n",
      "|  Bolívia|   Geração Y|     80445|\n",
      "|  Bolívia|   Geração Z|     80421|\n",
      "|   Brasil|Baby Boomers|    116711|\n",
      "|   Brasil|   Geração X|     87445|\n",
      "|   Brasil|   Geração Y|     88694|\n",
      "|   Brasil|   Geração Z|     87927|\n",
      "|    Chile|Baby Boomers|    128294|\n",
      "|    Chile|   Geração X|     95769|\n",
      "|    Chile|   Geração Y|     96368|\n",
      "|    Chile|   Geração Z|     96043|\n",
      "| Colômbia|Baby Boomers|    139894|\n",
      "| Colômbia|   Geração X|    104290|\n",
      "| Colômbia|   Geração Y|    104756|\n",
      "| Colômbia|   Geração Z|    104390|\n",
      "+---------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
    "\n",
    "query_geracoes = spark.sql(\"\"\"\n",
    "    select pais, \n",
    "           case when AnoNascimento between 1944 and 1964 then 'Baby Boomers'\n",
    "                when AnoNascimento between 1965 and 1979 then 'Geração X'\n",
    "                when AnoNascimento between 1980 and 1994 then 'Geração Y'\n",
    "                when AnoNascimento between 1995 and 2015 then 'Geração Z'\n",
    "           end as Geracao,\n",
    "           count(*) AS Quantidade\n",
    "    from pessoas\n",
    "    group by Pais, Geracao\n",
    "    order by Pais, Geracao, Quantidade asc\n",
    "\"\"\" )\n",
    "\n",
    "query_geracoes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
